{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "import librosa\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from transformers import Wav2Vec2Processor, Wav2Vec2Model\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch.nn.functional as F\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_song(mp3_dir, song_name):\n",
    "    # Load the mp3 file (Librosa loads audio as a NumPy array)\n",
    "    y, sr = librosa.load(os.path.join(mp3_dir, song_name), sr=None)\n",
    "\n",
    "    # Compute the Short-Time Fourier Transform (STFT)\n",
    "    D = librosa.stft(y)\n",
    "\n",
    "    # Convert the amplitude to decibels\n",
    "    S_db = librosa.amplitude_to_db(np.abs(D), ref=np.max)\n",
    "\n",
    "    # Create the plot\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    librosa.display.specshow(S_db, sr=sr, x_axis='time', y_axis='hz')\n",
    "    plt.colorbar(format='%+2.0f dB')\n",
    "    plt.title('Spectrogram of the song')\n",
    "    plt.xlabel('Time')\n",
    "    plt.ylabel('Frequency (Hz)')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hyarrava/.local/lib/python3.11/site-packages/transformers/configuration_utils.py:302: UserWarning: Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 Transformers. Using `model.gradient_checkpointing_enable()` instead, or if you are using the `Trainer` API, pass `gradient_checkpointing=True` in your `TrainingArguments`.\n",
      "  warnings.warn(\n",
      "/home/hyarrava/.local/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "wav2vec2_processor = Wav2Vec2Processor.from_pretrained(\"facebook/wav2vec2-base\")\n",
    "wav2vec2_model = Wav2Vec2Model.from_pretrained(\"facebook/wav2vec2-base\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_audio_embeddings(file_path,wav2vec2_model, wav2vec2_processor):\n",
    "    # Load audio file\n",
    "    print(file_path)\n",
    "    audio, sr = librosa.load(file_path, sr=16000)  # Use 16kHz as Wav2Vec2 expects 16kHz input\n",
    "    input_values = wav2vec2_processor(audio, return_tensors=\"pt\", sampling_rate=16000).input_values\n",
    "\n",
    "    # Pass through Wav2Vec2 model to get hidden-states (embeddings)\n",
    "    with torch.no_grad():\n",
    "        embeddings = wav2vec2_model(input_values).last_hidden_state\n",
    "    \n",
    "    # Convert embeddings to numpy array (optional)\n",
    "    embeddings_np = embeddings.squeeze(0).cpu().numpy()\n",
    "    \n",
    "    return embeddings_np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/hyarrava/Song_generator/data/DSP_LOVE_SONGS/dsp_love_song_14.mp3\n"
     ]
    }
   ],
   "source": [
    "file_path = \"/home/hyarrava/Song_generator/data/DSP_LOVE_SONGS/dsp_love_song_14.mp3\"\n",
    "embeddings = extract_audio_embeddings(file_path, wav2vec2_model, wav2vec2_processor)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to extract embeddings for all songs in a folder and save them\n",
    "def extract_embeddings_for_folder(folder_path):\n",
    "    embeddings_folder = os.path.join(folder_path, \"embeddings\")\n",
    "    \n",
    "    # Create the embeddings folder if it does not exist\n",
    "    if not os.path.exists(embeddings_folder):\n",
    "        os.makedirs(embeddings_folder)\n",
    "    \n",
    "    # Iterate over all mp3 files in the specified folder\n",
    "    for song_file in os.listdir(folder_path):\n",
    "        if song_file.endswith('.mp3'):\n",
    "            file_path = os.path.join(folder_path, song_file)\n",
    "            embeddings = extract_audio_embeddings(file_path)\n",
    "            \n",
    "            # Save embeddings as .npy file\n",
    "            embedding_file_path = os.path.join(embeddings_folder, f'{os.path.splitext(song_file)[0]}_embeddings.npy')\n",
    "            np.save(embedding_file_path, embeddings)\n",
    "            print(f\"Saved embeddings for {song_file} at {embedding_file_path}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the folder path for the songs\n",
    "#music_folder = \"/home/hyarrava/Song_generator/data/THAMAN_HIT_SONGS\"\n",
    "#extract_embeddings_for_folder(music_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Connecting the Embeddings folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_path = \"/home/hyarrava/Song_generator/data/\"\n",
    "DSP_LOVE_SONGS_FOLDER = \"DSP_LOVE_SONGS/embeddings/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_pca_embeddings(folder_path, dir_genre_folder):\n",
    "    full_path = os.path.join(folder_path, dir_genre_folder)\n",
    "    for file_name in os.listdir(full_path):\n",
    "        file_path = os.path.join(full_path,file_name)\n",
    "    \n",
    "        embedding  = np.load(file_path)\n",
    "        pca = PCA(n_components=10)\n",
    "        pca_embedding = pca.fit_transform(embedding)\n",
    "        pca_embedding_file_path = os.path.join(full_path, f'{os.path.splitext(file_path)[0]}_pca.npy')\n",
    "        np.save(pca_embedding_file_path, pca_embedding)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def truncate_or_pad(embedding, target_length=1000):\n",
    "    # Get current length of the embedding\n",
    "    current_length = embedding.shape[0]\n",
    "    \n",
    "    if current_length > target_length:\n",
    "        # Truncate the embedding if it's too long\n",
    "        return embedding[:target_length, :]\n",
    "    else:\n",
    "        # Pad the embedding if it's too short\n",
    "        padding = np.zeros((target_length - current_length, embedding.shape[1]))\n",
    "        return np.vstack((embedding, padding))\n",
    "\n",
    "\n",
    "def convert_into_short_embeddings(emb_vector, target_length = 1000):\n",
    "    current_length = emb_vector.shape[0]\n",
    "    factor = current_length//target_length\n",
    "\n",
    "    if factor >1:\n",
    "        return np.mean(emb_vector[:target_length*factor].reshape(target_length, \n",
    "                                                                 factor, emb_vector.shape[1]), axis=1)\n",
    "\n",
    "    else :\n",
    "        truncate_or_pad(emb_vector, target_length=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def short_embeddings(folder_path, dir_genre_folder):\n",
    "    full_path = os.path.join(folder_path, dir_genre_folder)\n",
    "    #stacked_songs_embeddings = np.array()\n",
    "    for file_name in os.listdir(full_path):\n",
    "        if file_name.split(\".\")[0].endswith(\"embeddings_pca\"):\n",
    "            file_path = os.path.join(full_path,file_name)\n",
    "            embedding  = np.load(file_path)\n",
    "            short_embeddings = convert_into_short_embeddings(embedding, target_length)\n",
    "            print(short_embeddings.shape)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_embeddings(folder_path, dsp_folder, thaman_folder, target_length):\n",
    "    dsp_songs = []\n",
    "    thaman_songs = []\n",
    "\n",
    "    for file_name in os.listdir(os.path.join(folder_path, dsp_folder)):\n",
    "        if file_name.split(\".\")[0].endswith(\"embeddings_pca\"):\n",
    "            file_path = os.path.join(folder_path, dsp_folder,file_name)\n",
    "            embedding  = np.load(file_path)\n",
    "            short_embeddings = convert_into_short_embeddings(embedding, target_length)\n",
    "            dsp_songs.append(short_embeddings)\n",
    "\n",
    "    for file_name in os.listdir(os.path.join(folder_path, thaman_folder)):\n",
    "        if file_name.split(\".\")[0].endswith(\"embeddings_pca\"):\n",
    "            file_path = os.path.join(folder_path,thaman_folder, file_name)\n",
    "            embedding  = np.load(file_path)\n",
    "            short_embeddings = convert_into_short_embeddings(embedding, target_length)\n",
    "            thaman_songs.append(short_embeddings)\n",
    "\n",
    "    X = np.array(dsp_songs + thaman_songs)  # Combine all embeddings\n",
    "    y = np.array([0] * len(dsp_songs) + [1] * len(thaman_songs))  # Label: 0 for DSP, 1 for Thaman\n",
    "    \n",
    "    return X, y\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_path = \"/home/hyarrava/Song_generator/data/\"\n",
    "dsp_folder = \"DSP_LOVE_SONGS/embeddings/\"\n",
    "thaman_folder = \"THAMAN_LOVE_SONGS/embeddings/\"\n",
    "target_length = 1000\n",
    "X, y = create_embeddings(folder_path, dsp_folder, thaman_folder, target_length)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Data Shape: (50, 1000, 10)\n",
      "Test Data Shape: (13, 1000, 10)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "print(\"Train Data Shape:\", X_train.shape)\n",
    "print(\"Test Data Shape:\", X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SongClassificationModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SongClassificationModel, self).__init__()\n",
    "        \n",
    "        # Define the layers\n",
    "        self.flatten = nn.Flatten()  # Flatten the (1000, 10) input to (10000,)\n",
    "        self.fc1 = nn.Linear(10000, 128)  # First Dense layer (input: 10000, output: 128)\n",
    "        self.dropout_layer1 = nn.Dropout(0.2)\n",
    "        self.fc2 = nn.Linear(128, 64)     # Second Dense layer (input: 128, output: 64)\n",
    "        self.dropout_layer2 = nn.Dropout(0.2)\n",
    "        self.fc3 = nn.Linear(64, 32)      # Third Dense layer (input: 64, output: 32)\n",
    "        self.fc4 = nn.Linear(32, 16)       # Output layer (input: 32, output: 1)\n",
    "        self.fc5 = nn.Linear(16, 1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        x = F.relu(self.fc1(x))  # ReLU activation\n",
    "        x = self.dropout_layer1(x)\n",
    "        x = F.relu(self.fc2(x))  # ReLU activation\n",
    "        x = self.dropout_layer2(x)\n",
    "        x = F.relu(self.fc3(x))  # ReLU activation\n",
    "        x = F.relu(self.fc4(x))\n",
    "        x = torch.sigmoid(self.fc5(x))  # Sigmoid for binary classification output\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_dataset(X_train, y_train, X_test, y_test, batch_size=8):\n",
    "    # Convert numpy arrays to PyTorch tensors\n",
    "    X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "    y_train_tensor = torch.tensor(y_train, dtype=torch.float32).unsqueeze(1)  # Add extra dimension for target\n",
    "    X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "    y_test_tensor = torch.tensor(y_test, dtype=torch.float32).unsqueeze(1)\n",
    "    \n",
    "    # Create DataLoader for batch processing\n",
    "    train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "    test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "    return train_loader, test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training function with checkpoint saving\n",
    "def train_model_with_checkpoint(model, train_loader, criterion, optimizer, num_epochs=20, checkpoint_dir='checkpoints/'):\n",
    "    # Create a directory to save checkpoints if it doesn't exist\n",
    "    if not os.path.exists(checkpoint_dir):\n",
    "        os.makedirs(checkpoint_dir)\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        running_loss = 0.0\n",
    "        for inputs, labels in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "        \n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss / len(train_loader):.4f}')\n",
    "        \n",
    "        # Save the model checkpoint after every epoch\n",
    "        checkpoint_path = os.path.join(checkpoint_dir, f'model_epoch_{epoch+1}.pth')\n",
    "        torch.save({\n",
    "            'epoch': epoch + 1,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'loss': running_loss / len(train_loader),\n",
    "        }, checkpoint_path)\n",
    "        print(f'Model checkpoint saved at {checkpoint_path}')\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, test_loader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_loader:\n",
    "            outputs = model(inputs)\n",
    "            predicted = (outputs > 0.5).float()  # Binary classification threshold\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    \n",
    "    accuracy = 100 * correct / total\n",
    "    print(f'Test Accuracy: {accuracy:.2f}%')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/30], Loss: 0.6875\n",
      "Model checkpoint saved at checkpoints/model_epoch_1.pth\n",
      "Epoch [2/30], Loss: 0.5301\n",
      "Model checkpoint saved at checkpoints/model_epoch_2.pth\n",
      "Epoch [3/30], Loss: 0.3160\n",
      "Model checkpoint saved at checkpoints/model_epoch_3.pth\n",
      "Epoch [4/30], Loss: 0.1844\n",
      "Model checkpoint saved at checkpoints/model_epoch_4.pth\n",
      "Epoch [5/30], Loss: 0.0514\n",
      "Model checkpoint saved at checkpoints/model_epoch_5.pth\n",
      "Epoch [6/30], Loss: 0.0135\n",
      "Model checkpoint saved at checkpoints/model_epoch_6.pth\n",
      "Epoch [7/30], Loss: 0.0035\n",
      "Model checkpoint saved at checkpoints/model_epoch_7.pth\n",
      "Epoch [8/30], Loss: 0.0032\n",
      "Model checkpoint saved at checkpoints/model_epoch_8.pth\n",
      "Epoch [9/30], Loss: 0.0034\n",
      "Model checkpoint saved at checkpoints/model_epoch_9.pth\n",
      "Epoch [10/30], Loss: 0.0005\n",
      "Model checkpoint saved at checkpoints/model_epoch_10.pth\n",
      "Epoch [11/30], Loss: 0.0010\n",
      "Model checkpoint saved at checkpoints/model_epoch_11.pth\n",
      "Epoch [12/30], Loss: 0.0007\n",
      "Model checkpoint saved at checkpoints/model_epoch_12.pth\n",
      "Epoch [13/30], Loss: 0.0033\n",
      "Model checkpoint saved at checkpoints/model_epoch_13.pth\n",
      "Epoch [14/30], Loss: 0.0002\n",
      "Model checkpoint saved at checkpoints/model_epoch_14.pth\n",
      "Epoch [15/30], Loss: 0.0003\n",
      "Model checkpoint saved at checkpoints/model_epoch_15.pth\n",
      "Epoch [16/30], Loss: 0.0004\n",
      "Model checkpoint saved at checkpoints/model_epoch_16.pth\n",
      "Epoch [17/30], Loss: 0.0001\n",
      "Model checkpoint saved at checkpoints/model_epoch_17.pth\n",
      "Epoch [18/30], Loss: 0.0001\n",
      "Model checkpoint saved at checkpoints/model_epoch_18.pth\n",
      "Epoch [19/30], Loss: 0.0006\n",
      "Model checkpoint saved at checkpoints/model_epoch_19.pth\n",
      "Epoch [20/30], Loss: 0.0001\n",
      "Model checkpoint saved at checkpoints/model_epoch_20.pth\n",
      "Epoch [21/30], Loss: 0.0003\n",
      "Model checkpoint saved at checkpoints/model_epoch_21.pth\n",
      "Epoch [22/30], Loss: 0.0003\n",
      "Model checkpoint saved at checkpoints/model_epoch_22.pth\n",
      "Epoch [23/30], Loss: 0.0003\n",
      "Model checkpoint saved at checkpoints/model_epoch_23.pth\n",
      "Epoch [24/30], Loss: 0.0002\n",
      "Model checkpoint saved at checkpoints/model_epoch_24.pth\n",
      "Epoch [25/30], Loss: 0.0000\n",
      "Model checkpoint saved at checkpoints/model_epoch_25.pth\n",
      "Epoch [26/30], Loss: 0.0003\n",
      "Model checkpoint saved at checkpoints/model_epoch_26.pth\n",
      "Epoch [27/30], Loss: 0.0005\n",
      "Model checkpoint saved at checkpoints/model_epoch_27.pth\n",
      "Epoch [28/30], Loss: 0.0003\n",
      "Model checkpoint saved at checkpoints/model_epoch_28.pth\n",
      "Epoch [29/30], Loss: 0.0002\n",
      "Model checkpoint saved at checkpoints/model_epoch_29.pth\n",
      "Epoch [30/30], Loss: 0.0005\n",
      "Model checkpoint saved at checkpoints/model_epoch_30.pth\n",
      "Test Accuracy: 69.23%\n"
     ]
    }
   ],
   "source": [
    "batch_size = 8\n",
    "num_epochs = 30\n",
    "learning_rate = 0.001\n",
    "\n",
    "# Prepare data (assuming X_train, y_train, X_test, y_test are already prepared as numpy arrays)\n",
    "train_loader, test_loader = prepare_dataset(X_train, y_train, X_test, y_test, batch_size)\n",
    "\n",
    "# Initialize model, criterion (loss function), and optimizer\n",
    "model = SongClassificationModel()\n",
    "criterion = nn.BCELoss()  # Binary cross-entropy loss for binary classification\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Train the model\n",
    "model = train_model_with_checkpoint(model, train_loader, criterion, optimizer, num_epochs)\n",
    "torch.save(model.state_dict(), 'model_weights.pth')\n",
    "# Evaluate the model\n",
    "evaluate_model(model, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('X_train.npy', X_train)\n",
    "np.save('y_train.npy', y_train)\n",
    "\n",
    "# If you have test data\n",
    "np.save('X_test.npy', X_test)\n",
    "np.save('y_test.npy', y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test unknown audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/hyarrava/Song_generator/notebooks/videoplayback.mp3\n"
     ]
    }
   ],
   "source": [
    "test_audio_path = \"/home/hyarrava/Song_generator/notebooks/videoplayback.mp3\"\n",
    "test_embeddings = extract_audio_embeddings(test_audio_path, wav2vec2_model, wav2vec2_processor)\n",
    "pca = PCA(n_components=10)\n",
    "test_pca_embedding = pca.fit_transform(test_embeddings)\n",
    "short_pca_emb = convert_into_short_embeddings(test_pca_embedding, target_length = 1000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "test_input = torch.tensor(short_pca_emb, dtype = torch.float32)\n",
    "test_tensor = test_input.permute(1,0).unsqueeze(0) \n",
    "\n",
    "with torch.no_grad():\n",
    "    output = model(test_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Label: DSP\n"
     ]
    }
   ],
   "source": [
    "predicted_label = (output > 0.5).float()\n",
    "if predicted_label.item() == 1.0:\n",
    "    print(\"Predicted Label: DSP\")\n",
    "else:\n",
    "    print(\"Predicted Label: Thaman\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.6272]])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
